from optimizer import Optimizer


class SGD(Optimizer):
    '''
    Stochasitc gradient descent optimizer.
    '''    
    def __init__(self, learning_rate: float = 0.01) -> None:
        '''Pass'''
        super().__init__(learning_rate)

    def step(self):
        '''
        For each parameter, adjust in the appropriate direction, with the magnitude of the adjustment 
        based on the learning rate.
        '''
        for (param, param_grad) in zip(self.net.params(), self.net.param_grads()):
            param -= self.learning_rate * param_grad